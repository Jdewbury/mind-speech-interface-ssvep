{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction to Steady-state Visual Evoked Potentials (SSVEP) Brain-Computer Interfaces (BCI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brain Computer Interfaces (BCIs)\n",
    "Brain-computer interfaces (BCIs) provide a direct pathway between the human brain and an external device, and can be especially useful for people with severe disabilities. A BCI can bypass the damaged pathways in order to provide the user with the ability to communicate or interact effectively with their surroundings. BCIs provide novel possibilities for neurorehabilitation for people with neurological disease such as stroke, amyotrophic lateral sclerosis (ALS) or paralysis.\n",
    "\n",
    "There are two types of BCIs: endogenous and exogenous BCIs. Endogenous BCIs allows the user to voluntarily modulate\n",
    "his/her neuronal activity based on intention. For example: Motor Imagery (MI BCI) â€“ the user imagines a motor\n",
    "movement such as moving the right hand/left hand or imagines lifting up the ankle (ankle\n",
    "dorsiflexion). Exogenous BCIs are based on responses elicited due to an external stimulus. These responses are generated when the user focuses his/her attention on a stimulus which is associated with a BCI command. Examples include P300 BCI, steady-state visual evoked potentials (SSVEP) based BCIs, steady-state motion visual evoked potential (SSMVEP) BCIs, etc. \n",
    "\n",
    "\n",
    "#### Steady-state Visual evoked Potentials (SSVEP) based BCI\n",
    "\n",
    "Steady-state visual evoked potentials are responses elicited when a user focuses his/her attention on a repetitive visual stimulus (a light source) flickering at frequencies higher than 6 Hz. These are periodic responses prominently observed in the occipital and occipito-parietal areas of the cortex. SSVEP responses appear as an increase in the amplitude of the signal at the fundamental frequency and its harmonics for the corresponding stimulus attended by the user. Therefore, by analyzing the dominant frequency response in the EEG, the stimulus attended to by the user can be identified. \n",
    "\n",
    "Video: https://youtu.be/cd4m2flXS2U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass_filter(data, lowcut, highcut, sample_rate, order):\n",
    "    '''\n",
    "    Returns bandpass filtered data between the frequency ranges specified in the input.\n",
    "    Args:\n",
    "        data (numpy.ndarray): array of samples. \n",
    "        lowcut (float): lower cutoff frequency (Hz).\n",
    "        highcut (float): lower cutoff frequency (Hz).\n",
    "        sample_rate (float): sampling rate (Hz).\n",
    "        order (int): order of the bandpass filter.\n",
    "    Returns:\n",
    "        (numpy.ndarray): bandpass filtered data.\n",
    "    '''\n",
    "    \n",
    "    nyq = 0.5 * sample_rate\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no overlapping buffer, were keeping it simple for now (ask aravind later)\n",
    "def get_filtered_eeg(eeg, lowcut, highcut, order, sample_rate):\n",
    "    '''\n",
    "    Returns bandpass filtered eeg for all channels and trials.\n",
    "    Args:\n",
    "        eeg (numpy.ndarray): raw eeg data of shape (num_classes, num_channels, num_samples, num_trials).\n",
    "        lowcut (float): lower cutoff frequency (Hz).\n",
    "        highcut (float): lower cutoff frequency (Hz).\n",
    "        order (int): order of the bandpass filter.\n",
    "        sample_rate (float): sampling rate (Hz).\n",
    "    Returns:\n",
    "        (numpy.ndarray): bandpass filtered eeg of shape (num_classes, num_channels, num_samples, num_trials).\n",
    "    '''\n",
    "    \n",
    "    num_classes = eeg.shape[0]\n",
    "    num_chan = eeg.shape[1]\n",
    "    total_trial_len = eeg.shape[2]\n",
    "    num_trials = eeg.shape[3]\n",
    "\n",
    "    #instantiate object to be sent to BP filter\n",
    "    filtered_data = np.zeros((eeg.shape[0], eeg.shape[1], total_trial_len, eeg.shape[3]))\n",
    "\n",
    "    \n",
    "    for target in range(0, num_classes):\n",
    "        for channel in range(0, num_chan):\n",
    "            for trial in range(0, num_trials):\n",
    "                \n",
    "                #data to be filtered\n",
    "                signal_to_filter = np.squeeze( eeg[target, channel, 0:total_trial_len, trial] )\n",
    "                \n",
    "                #call to BP filter\n",
    "                filtered_data[target, channel, :, trial] = butter_bandpass_filter(signal_to_filter, \n",
    "                                                                                  lowcut, highcut, \n",
    "                                                                                  sample_rate, order)\n",
    "                \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer(data, duration, data_overlap):\n",
    "    '''\n",
    "    Returns segmented data based on the provided input window duration and overlap.\n",
    "    Args:\n",
    "        data (numpy.ndarray): array of samples. \n",
    "        duration (int): window length (number of samples).\n",
    "        data_overlap (int): number of samples of overlap.\n",
    "    Returns:\n",
    "        (numpy.ndarray): segmented data of shape (number_of_segments, duration).\n",
    "    '''\n",
    "    \n",
    "    number_segments = int(np.ceil((len(data) - data_overlap)/(duration - data_overlap)))\n",
    "    temp_buf = [data[i:i+duration] for i in range(0, len(data), (duration - int(data_overlap)))]\n",
    "    temp_buf[number_segments-1] = np.pad(temp_buf[number_segments-1],\n",
    "                                         (0, duration-temp_buf[number_segments-1].shape[0]),\n",
    "                                         'constant')\n",
    "    segmented_data = np.vstack(temp_buf[0:number_segments])\n",
    "    \n",
    "    return segmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmented_epochs(data, window_len, shift_len, sample_rate):\n",
    "    '''\n",
    "    Returns epoched eeg data based on the window duration and step size.\n",
    "    Args:\n",
    "        data (numpy.ndarray): array of samples. \n",
    "        window_len (int): window length (seconds).\n",
    "        shift_len (int): step size (seconds).\n",
    "        sample_rate (float): sampling rate (Hz).\n",
    "    Returns:\n",
    "        (numpy.ndarray): epoched eeg data of shape. \n",
    "        (num_classes, num_channels, num_trials, number_of_segments, duration).\n",
    "    '''\n",
    "    \n",
    "    num_classes = data.shape[0]\n",
    "    num_chan = data.shape[1]\n",
    "    num_trials = data.shape[3]\n",
    "    \n",
    "    duration = int(window_len*sample_rate)\n",
    "    data_overlap = (window_len - shift_len)*sample_rate\n",
    "    \n",
    "    number_of_segments = int(np.ceil((data.shape[2] - data_overlap)/\n",
    "                                       (duration - data_overlap)))\n",
    "    \n",
    "    segmented_data = np.zeros((data.shape[0], data.shape[1], \n",
    "                               data.shape[3], number_of_segments, duration))\n",
    "\n",
    "    for target in range(0, num_classes):\n",
    "        for channel in range(0, num_chan):\n",
    "            for trial in range(0, num_trials):\n",
    "                segmented_data[target, channel, trial, :, :] = buffer(data[target, channel, :, trial], \n",
    "                                                                      duration, data_overlap) \n",
    "    \n",
    "    return segmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_spectrum_features(segmented_data, FFT_PARAMS):\n",
    "    '''\n",
    "    Returns magnitude spectrum features. Fast Fourier Transform computed based on\n",
    "    the FFT parameters provided as input.\n",
    "    Args:\n",
    "        segmented_data (numpy.ndarray): epoched eeg data of shape \n",
    "        (num_classes, num_channels, num_trials, number_of_segments, num_samples).\n",
    "        FFT_PARAMS (dict): dictionary of parameters used for feature extraction.\n",
    "        FFT_PARAMS['resolution'] (float): frequency resolution per bin (Hz).\n",
    "        FFT_PARAMS['start_frequency'] (float): start frequency component to pick from (Hz). \n",
    "        FFT_PARAMS['end_frequency'] (float): end frequency component to pick upto (Hz). \n",
    "        FFT_PARAMS['sampling_rate'] (float): sampling rate (Hz).\n",
    "    Returns:\n",
    "        (numpy.ndarray): magnitude spectrum features of the input EEG.\n",
    "        (n_fc, num_channels, num_classes, num_trials, number_of_segments).\n",
    "    '''\n",
    "    \n",
    "    num_classes = segmented_data.shape[0]\n",
    "    num_chan = segmented_data.shape[1]\n",
    "    num_trials = segmented_data.shape[2]\n",
    "    number_of_segments = segmented_data.shape[3]\n",
    "    fft_len = segmented_data[0, 0, 0, 0, :].shape[0]\n",
    "\n",
    "    NFFT = round(FFT_PARAMS['sampling_rate']/FFT_PARAMS['resolution'])\n",
    "    fft_index_start = int(round(FFT_PARAMS['start_frequency']/FFT_PARAMS['resolution']))\n",
    "    fft_index_end = int(round(FFT_PARAMS['end_frequency']/FFT_PARAMS['resolution']))+1\n",
    "\n",
    "    features_data = np.zeros(((fft_index_end - fft_index_start), \n",
    "                              segmented_data.shape[1], segmented_data.shape[0], \n",
    "                              segmented_data.shape[2], segmented_data.shape[3]))\n",
    "    \n",
    "    for target in range(0, num_classes):\n",
    "        for channel in range(0, num_chan):\n",
    "            for trial in range(0, num_trials):\n",
    "                for segment in range(0, number_of_segments):\n",
    "                    temp_FFT = np.fft.fft(segmented_data[target, channel, trial, segment, :], NFFT)/fft_len\n",
    "                    magnitude_spectrum = 2*np.abs(temp_FFT)\n",
    "                    features_data[:, channel, target, trial, segment] = magnitude_spectrum[fft_index_start:fft_index_end,]\n",
    "    \n",
    "    return features_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_spectrum_features(segmented_data, FFT_PARAMS):\n",
    "    '''\n",
    "    Returns complex spectrum features. Fast Fourier Transform computed based on\n",
    "    the FFT parameters provided as input. The real and imaginary parts of the input\n",
    "    signal are concatenated into a single feature vector.\n",
    "    Args:\n",
    "        segmented_data (numpy.ndarray): epoched eeg data of shape \n",
    "        (num_classes, num_channels, num_trials, number_of_segments, num_samples).\n",
    "        FFT_PARAMS (dict): dictionary of parameters used for feature extraction.\n",
    "        FFT_PARAMS['resolution'] (float): frequency resolution per bin (Hz).\n",
    "        FFT_PARAMS['start_frequency'] (float): start frequency component to pick from (Hz). \n",
    "        FFT_PARAMS['end_frequency'] (float): end frequency component to pick upto (Hz). \n",
    "        FFT_PARAMS['sampling_rate'] (float): sampling rate (Hz).\n",
    "    Returns:\n",
    "        (numpy.ndarray): complex spectrum features of the input EEG.\n",
    "        (2*n_fc, num_channels, num_classes, num_trials, number_of_segments)\n",
    "    '''\n",
    "    \n",
    "    num_classes = segmented_data.shape[0]\n",
    "    num_chan = segmented_data.shape[1]\n",
    "    num_trials = segmented_data.shape[2]\n",
    "    number_of_segments = segmented_data.shape[3]\n",
    "    fft_len = segmented_data[0, 0, 0, 0, :].shape[0]\n",
    "\n",
    "    NFFT = round(FFT_PARAMS['sampling_rate']/FFT_PARAMS['resolution'])\n",
    "    fft_index_start = int(round(FFT_PARAMS['start_frequency']/FFT_PARAMS['resolution']))\n",
    "    fft_index_end = int(round(FFT_PARAMS['end_frequency']/FFT_PARAMS['resolution']))+1\n",
    "\n",
    "    features_data = np.zeros((2*(fft_index_end - fft_index_start), \n",
    "                              segmented_data.shape[1], segmented_data.shape[0], \n",
    "                              segmented_data.shape[2], segmented_data.shape[3]))\n",
    "    \n",
    "    for target in range(0, num_classes):\n",
    "        for channel in range(0, num_chan):\n",
    "            for trial in range(0, num_trials):\n",
    "                for segment in range(0, number_of_segments):\n",
    "                    temp_FFT = np.fft.fft(segmented_data[target, channel, trial, segment, :], NFFT)/fft_len\n",
    "                    real_part = np.real(temp_FFT)\n",
    "                    imag_part = np.imag(temp_FFT)\n",
    "                    features_data[:, channel, target, trial, segment] = np.concatenate((\n",
    "                        real_part[fft_index_start:fft_index_end,], \n",
    "                        imag_part[fft_index_start:fft_index_end,]), axis=0)\n",
    "    \n",
    "    return features_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up some completely fixed parameters\n",
    "FFT_PARAMS = {\n",
    "    'resolution': 0.2930,\n",
    "    'start_frequency': 0.0,\n",
    "    'end_frequency': 35.0,\n",
    "    'sampling_rate': 250\n",
    "}\n",
    "data_path = os.path.abspath('../data')\n",
    "magnitude_spectrum = dict()\n",
    "window_len = 4.4\n",
    "shift_len = 4.4\n",
    "sample_rate = FFT_PARAMS['sampling_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data Set\n",
    "Load the dataset and compute the magnitude spectrum for every trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE SURE CSV AND NOTEBOOK ARE IN THE SAME DIRECTORY\n",
    "df = pd.read_csv (r'174_2022_123780.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing what flicker frequencies are present\n",
    "flicker_freq_dict = dict()\n",
    "\n",
    "# Adding row keys of relevant frequencies from dataframe \n",
    "for i, freq_point in enumerate(df['Frequency']):\n",
    "    if not np.isnan(freq_point) and not freq_point == 0:\n",
    "        if freq_point not in flicker_freq_dict.keys():\n",
    "            flicker_freq_dict.update({freq_point: [i]})\n",
    "        else:\n",
    "            flicker_freq_dict[freq_point].append(i)\n",
    "flicker_freq = np.array(list(flicker_freq_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(flicker_freq)\n",
    "#print(flicker_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#USE THIS TO SEE IF MARKER PLACEMENT WAS DELAYED, THIS IS ONLY USED FOR TESTING, OTHERWISE LEAVE OFFST = 0\n",
    "# offst  = 0\n",
    "# for i in range(len(flicker_freq_dict[9.25])):\n",
    "#     flicker_freq_dict[9.25][i] = flicker_freq_dict[9.25][i] + offst\n",
    "#     flicker_freq_dict[11.25][i] = flicker_freq_dict[11.25][i] + offst\n",
    "#     flicker_freq_dict[13.25][i] = flicker_freq_dict[13.25][i] + offst\n",
    "#     flicker_freq_dict[9.75][i] = flicker_freq_dict[9.75][i] + offst\n",
    "#     flicker_freq_dict[11.75][i] = flicker_freq_dict[11.75][i] + offst\n",
    "#     flicker_freq_dict[13.75][i] = flicker_freq_dict[13.75][i] + offst\n",
    "#     flicker_freq_dict[10.25][i] = flicker_freq_dict[10.25][i] + offst\n",
    "#     flicker_freq_dict[12.25][i] = flicker_freq_dict[12.25][i] + offst\n",
    "#     flicker_freq_dict[14.25][i] = flicker_freq_dict[14.25][i] + offst\n",
    "#     flicker_freq_dict[10.75][i] = flicker_freq_dict[10.75][i] + offst\n",
    "#     flicker_freq_dict[12.75][i] = flicker_freq_dict[12.75][i] + offst\n",
    "#     flicker_freq_dict[14.75][i] = flicker_freq_dict[14.75][i] + offst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(flicker_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the eeg data -> making the appropriate matrix\n",
    "# Initializing the dimensions of the eeg matrix\n",
    "\n",
    "num_classes = len(flicker_freq) \n",
    "n_ch = 8 \n",
    "total_trial_len = 1100 \n",
    "\n",
    "#scales to number of trials in csv for each freq\n",
    "num_trials = min(len(flicker_freq_dict[key]) for key in flicker_freq)\n",
    "\n",
    "#instantiates eeg data in 4 dimensional np array\n",
    "eeg = np.zeros((num_classes,n_ch,total_trial_len,num_trials))\n",
    "\n",
    "# Assigning the correct values to the matrix/object\n",
    "\n",
    "start_idx_list = []\n",
    "\n",
    "\n",
    "#grabs start and endpoints for each frequency flash\n",
    "for i, freq in enumerate(flicker_freq):\n",
    "    for j in range(num_trials):\n",
    "        start_idx = flicker_freq_dict[freq][j]\n",
    "        start_idx_list.append(start_idx)\n",
    "        end_idx = start_idx + total_trial_len\n",
    "        \n",
    "        #shaves off timestamps and markers and does a transpose, we transpose it back and cast as a np array\n",
    "        eeg[i, :, :, j] = np.array(df.iloc[start_idx:end_idx, 1:9]).transpose((1,0))\n",
    "\n",
    "#wrapper function for EEG data filtering with 4th order BP\n",
    "filtered_data = get_filtered_eeg(eeg, 9, 16, 4, sample_rate)\n",
    "\n",
    "\n",
    "\n",
    "segmented_data = get_segmented_epochs(filtered_data, window_len, shift_len, sample_rate)\n",
    "\n",
    "\n",
    "\n",
    "magnitude_spectrum = magnitude_spectrum_features(segmented_data, FFT_PARAMS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Response calculated using Fast-Fourier Transform (FFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject 1\n",
    "Example dataset containing good frequency responses for each flicker frequency.\n",
    "The flicker frequencies can be readily identified from the magnitude response of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_spectrum(ax, magnitude_spectrum, resolution, num_classes, channel, flicker_freq, title=None):\n",
    "    \n",
    "    for target in range(num_classes):\n",
    "        \n",
    "        fft_axis = np.arange(magnitude_spectrum.shape[0])*resolution\n",
    "        \n",
    "        ax[target].plot(fft_axis, np.mean(np.squeeze(magnitude_spectrum[:, channel, target, :, :]), \n",
    "                                          axis=1))\n",
    "        \n",
    "        #ax[target].plot(fft_axis, np.squeeze(magnitude_spectrum[:, channel, target, :, :]))\n",
    "        ax[target].set_xlabel('Frequency (Hz)') \n",
    "        ax[target].set_ylabel('Amplitude (uV)')\n",
    "        if title:\n",
    "            ax[target].set_title(f' freq. {flicker_freq[target]} Hz Ch: {title}')\n",
    "        else:\n",
    "            ax[target].set_title(f' freq. {flicker_freq[target]} Hz')\n",
    "        ax[target].set_xlim([5,20])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELECTRODE LOCATIONS (10 20 diagram) BASED ON CHANNEL \n",
    "channel_list = ['PO4', 'O2', 'Oz', 'POz', 'PO3', 'O1', 'PO8', 'PO7']\n",
    "\n",
    "# PLOT FFT RESPONSE\n",
    "for channel in range(7):\n",
    "    fig, axes = plt.subplots(2, 2,figsize=(16, 14), gridspec_kw=dict(hspace=0.45, wspace=0.3))\n",
    "    axes = axes.reshape(-1)\n",
    "    plot_spectrum(axes, magnitude_spectrum, FFT_PARAMS['resolution'], num_classes,\n",
    "                  channel, flicker_freq, title=channel_list[channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
